{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlFQXu6h5zfKgdigYK+mqj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saibaddala/DL_Assignment_1/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gozsKq6FKAUQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import wandb\n",
        "\n",
        "# End any open wandb sessions\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def acquire_fashion_mnist_data():\n",
        "    \"\"\"\n",
        "    Fetch Fashion MNIST, returning (train_imgs, train_lbls) and (test_imgs, test_lbls),\n",
        "    plus a sample set containing one image for each label.\n",
        "    \"\"\"\n",
        "    (train_imgs, train_lbls), (test_imgs, test_lbls) = fashion_mnist.load_data()\n",
        "\n",
        "    unique_tracker = np.unique(train_lbls)\n",
        "    sample_collection = []\n",
        "\n",
        "    # Pick one sample image per class\n",
        "    for im, lab in zip(train_imgs, train_lbls):\n",
        "        if lab in unique_tracker:\n",
        "            sample_collection.append(im)\n",
        "            location = np.where(unique_tracker == lab)\n",
        "            unique_tracker = np.delete(unique_tracker, location)\n",
        "        if len(unique_tracker) == 0:\n",
        "            break\n",
        "\n",
        "    return (train_imgs, train_lbls), (test_imgs, test_lbls), sample_collection"
      ],
      "metadata": {
        "id": "3_P0G4hkKvdG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tr_images, tr_labels, ts_images, ts_labels, ratio=0.9):\n",
        "    \"\"\"\n",
        "    Flatten 28x28 images into 1D, normalize pixel values, and split training\n",
        "    into train and validation sets according to ratio.\n",
        "    \"\"\"\n",
        "    total_train_count = len(tr_images)\n",
        "    valid_start_index = int(total_train_count * ratio)\n",
        "\n",
        "    all_flat_train = []\n",
        "    for i in range(total_train_count):\n",
        "        flat_image = tr_images[i].reshape(-1) / 255.0\n",
        "        all_flat_train.append(flat_image)\n",
        "\n",
        "    all_flat_test = []\n",
        "    for j in range(len(ts_images)):\n",
        "        flattened_test = ts_images[j].flatten() / 255.0\n",
        "        all_flat_test.append(flattened_test)\n",
        "\n",
        "    xtrain = np.array(all_flat_train[:valid_start_index])\n",
        "    ytrain = tr_labels[:valid_start_index]\n",
        "    xval   = np.array(all_flat_train[valid_start_index:])\n",
        "    yval   = tr_labels[valid_start_index:]\n",
        "    xtest  = np.array(all_flat_test)\n",
        "    ytest  = ts_labels\n",
        "\n",
        "    return xtrain, ytrain, xval, yval, xtest, ytest"
      ],
      "metadata": {
        "id": "TRSOZigpK1pF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_class_samples(sample_pics, label_names):\n",
        "    \"\"\"\n",
        "    Plot sample images (one per class) and log the figure to W&B.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax_count = 2 * 5  # 2 rows, 5 cols\n",
        "\n",
        "    for idx in range(ax_count):\n",
        "        subp = fig.add_subplot(2, 5, idx + 1)\n",
        "        subp.imshow(sample_pics[idx], cmap='binary')\n",
        "        subp.set_title(label_names[idx])\n",
        "        subp.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_examples.png')\n",
        "\n",
        "    wandb.login()\n",
        "    wandb.init(project='DL assignment 1')\n",
        "    wandb.log({\"sample_images\": wandb.Image('class_examples.png')})\n",
        "\n",
        "    plt.show()\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "2mAKJYFeK9YY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    A multi-layer neural network with flexible initialization, activations,\n",
        "    optimizers, and loss functions.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_outputs, cfg):\n",
        "        self.in_features   = input_size\n",
        "        self.out_features  = num_outputs\n",
        "        self.hidden_count  = cfg[\"hidden_layers\"]\n",
        "        self.hidden_dim    = cfg[\"hl_size\"]\n",
        "        self.total_layers  = self.hidden_count + 1\n",
        "\n",
        "        self.init_approach  = cfg[\"initialization\"]\n",
        "        self.activ_method   = cfg[\"activation\"]\n",
        "        self.loss_mech      = cfg[\"loss\"]\n",
        "        self.optimizer_kind = cfg[\"optimizer\"]\n",
        "        self.lr             = cfg[\"learning_rate\"]\n",
        "        self.weight_decay   = cfg[\"weight_decay\"]\n",
        "        self.batch_size     = cfg[\"batch_size\"]\n",
        "        self.epochs         = cfg[\"epochs\"]\n",
        "\n",
        "        self.momentum_factor = cfg.get(\"momentum_beta\", 0.9)\n",
        "        self.rms_factor      = cfg.get(\"rms_beta\", 0.5)\n",
        "        self.adam_beta1      = cfg.get(\"beta1\", 0.9)\n",
        "        self.adam_beta2      = cfg.get(\"beta2\", 0.9)\n",
        "        self.epsilon_adam    = cfg.get(\"eps\", 1e-8)\n",
        "\n",
        "        self.W_matrices = []\n",
        "        self.B_vectors  = []\n",
        "\n",
        "        self._prepare_parameters()\n",
        "\n",
        "    def _prepare_parameters(self):\n",
        "        for i in range(self.total_layers):\n",
        "            if i == 0:\n",
        "                in_dim = self.in_features\n",
        "                out_dim = self.hidden_dim\n",
        "            elif i == self.total_layers - 1:\n",
        "                in_dim = self.hidden_dim\n",
        "                out_dim = self.out_features\n",
        "            else:\n",
        "                in_dim = self.hidden_dim\n",
        "                out_dim = self.hidden_dim\n",
        "\n",
        "            if self.init_approach == \"random\":\n",
        "                W_temp = np.random.randn(out_dim, in_dim) * 0.01\n",
        "                B_temp = np.random.randn(out_dim, 1) * 0.01\n",
        "            else:  # Xavier\n",
        "                scale_factor = np.sqrt(2.0 / (in_dim + out_dim))\n",
        "                W_temp = np.random.randn(out_dim, in_dim) * scale_factor\n",
        "                B_temp = np.zeros((out_dim, 1))\n",
        "\n",
        "            self.W_matrices.append(W_temp)\n",
        "            self.B_vectors.append(B_temp)\n",
        "\n",
        "    def feed_forward_neural_net(self, single_input):\n",
        "        \"\"\"\n",
        "        Forward pass for one sample:\n",
        "        returns (layer_outputs, pre_activation_values).\n",
        "        \"\"\"\n",
        "        layer_outputs   = [None] * self.total_layers\n",
        "        pre_activations = [None] * self.total_layers\n",
        "        current_act     = single_input.reshape(-1, 1)\n",
        "\n",
        "        for idx in range(self.total_layers):\n",
        "            z_val = np.dot(self.W_matrices[idx], current_act) + self.B_vectors[idx]\n",
        "            pre_activations[idx] = z_val\n",
        "\n",
        "            if idx == self.total_layers - 1:\n",
        "                out_val = self._softmax(z_val)\n",
        "            else:\n",
        "                out_val = self._apply_activation(self.activ_method, z_val)\n",
        "\n",
        "            layer_outputs[idx] = out_val\n",
        "            current_act = out_val\n",
        "\n",
        "        return layer_outputs, pre_activations\n",
        "\n",
        "    def backward_eval(self, layer_outputs, pre_acts, true_label, x_vec):\n",
        "        \"\"\"\n",
        "        Backprop for one sample. Returns gradients for each layer's weights and biases.\n",
        "        \"\"\"\n",
        "        grad_weights = [np.zeros_like(wi) for wi in self.W_matrices]\n",
        "        grad_biases  = [np.zeros_like(bi) for bi in self.B_vectors]\n",
        "\n",
        "        # One-hot target\n",
        "        y_one_hot = np.zeros((self.out_features, 1))\n",
        "        y_one_hot[true_label] = 1\n",
        "\n",
        "        # Final layer derivative depends on loss\n",
        "        if self.loss_mech == \"cross_entropy\":\n",
        "            dz_final = (layer_outputs[-1] - y_one_hot)\n",
        "        else:  # MSE\n",
        "            diff = layer_outputs[-1] - y_one_hot\n",
        "            dz_final = diff * (layer_outputs[-1] * (1 - layer_outputs[-1]))\n",
        "\n",
        "        dz_vals = [None] * self.total_layers\n",
        "        dz_vals[self.total_layers - 1] = dz_final\n",
        "\n",
        "        for i in range(self.total_layers - 1, -1, -1):\n",
        "            if i == 0:\n",
        "                input_vec = x_vec.reshape(1, -1)\n",
        "            else:\n",
        "                input_vec = layer_outputs[i - 1].T\n",
        "\n",
        "            grad_weights[i] = np.dot(dz_vals[i], input_vec)\n",
        "            grad_biases[i]  = dz_vals[i]\n",
        "\n",
        "            if i > 0:\n",
        "                up_val = np.dot(self.W_matrices[i].T, dz_vals[i])\n",
        "                dz_vals[i - 1] = up_val * self._activate_derivative(self.activ_method, pre_acts[i - 1])\n",
        "\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    def training(self, x_train, y_train, x_val, y_val):\n",
        "        \"\"\"\n",
        "        Trains using whichever optimizer is set in the config.\n",
        "        \"\"\"\n",
        "        opt = self.optimizer_kind\n",
        "        if opt == \"sgd\":\n",
        "            self.sgd_optimizer(x_train, y_train, x_val, y_val)\n",
        "        elif opt == \"momentum\":\n",
        "            self.momentum_optimizer(x_train, y_train, x_val, y_val)\n",
        "        elif opt == \"nestrov\":\n",
        "            self.nesterov_optimizer(x_train, y_train, x_val, y_val)\n",
        "        elif opt == \"rmsprop\":\n",
        "            self.rmsprop_optimizer(x_train, y_train, x_val, y_val)\n",
        "        elif opt == \"adam\":\n",
        "            self.adam_optimizer(x_train, y_train, x_val, y_val)\n",
        "        elif opt == \"nadam\":\n",
        "            self.nadam_optimizer(x_train, y_train, x_val, y_val)\n",
        "\n",
        "    def sgd_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        Simple SGD with optional L2 regularization.\n",
        "        \"\"\"\n",
        "        total_data = len(xT)\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for start in range(0, total_data, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                x_batch = xT[start:end]\n",
        "                y_batch = yT[start:end]\n",
        "\n",
        "                dw_acc = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                db_acc = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for i_sample in range(len(x_batch)):\n",
        "                    outs, pres = self.forward_eval(x_batch[i_sample])\n",
        "                    d_w, d_b = self.backward_eval(outs, pres, y_batch[i_sample], x_batch[i_sample])\n",
        "                    for l_idx in range(self.total_layers):\n",
        "                        dw_acc[l_idx] += d_w[l_idx]\n",
        "                        db_acc[l_idx] += d_b[l_idx]\n",
        "\n",
        "                for l_idx in range(self.total_layers):\n",
        "                    self.W_matrices[l_idx] -= self.lr * dw_acc[l_idx] + self.weight_decay * self.W_matrices[l_idx]\n",
        "                    self.B_vectors[l_idx]  -= self.lr * db_acc[l_idx]\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def momentum_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        Momentum-based GD.\n",
        "        \"\"\"\n",
        "        vel_w = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        vel_b = [np.zeros_like(b) for b in self.B_vectors]\n",
        "        total_len = len(xT)\n",
        "\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for b_start in range(0, total_len, self.batch_size):\n",
        "                b_end = b_start + self.batch_size\n",
        "                x_batch = xT[b_start:b_end]\n",
        "                y_batch = yT[b_start:b_end]\n",
        "\n",
        "                gradW = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                gradB = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for idx in range(len(x_batch)):\n",
        "                    outs, pres = self.forward_eval(x_batch[idx])\n",
        "                    temp_w, temp_b = self.backward_eval(outs, pres, y_batch[idx], x_batch[idx])\n",
        "                    for l_idx in range(self.total_layers):\n",
        "                        gradW[l_idx] += temp_w[l_idx]\n",
        "                        gradB[l_idx] += temp_b[l_idx]\n",
        "\n",
        "                for l_idx in range(self.total_layers):\n",
        "                    vel_w[l_idx] = self.momentum_factor * vel_w[l_idx] + self.lr * gradW[l_idx]\n",
        "                    self.W_matrices[l_idx] -= vel_w[l_idx] + self.weight_decay * self.W_matrices[l_idx]\n",
        "\n",
        "                    vel_b[l_idx] = self.momentum_factor * vel_b[l_idx] + self.lr * gradB[l_idx]\n",
        "                    self.B_vectors[l_idx] -= vel_b[l_idx]\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def nesterov_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        Nesterov Accelerated Gradient.\n",
        "        \"\"\"\n",
        "        v_w = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        v_b = [np.zeros_like(b) for b in self.B_vectors]\n",
        "        length_data = len(xT)\n",
        "\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for start_idx in range(0, length_data, self.batch_size):\n",
        "                end_idx = start_idx + self.batch_size\n",
        "                subX = xT[start_idx:end_idx]\n",
        "                subY = yT[start_idx:end_idx]\n",
        "\n",
        "                # Look-ahead\n",
        "                for ly in range(self.total_layers):\n",
        "                    self.W_matrices[ly] -= self.momentum_factor * v_w[ly]\n",
        "                    self.B_vectors[ly]  -= self.momentum_factor * v_b[ly]\n",
        "\n",
        "                dw_sum = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                db_sum = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for i_samp in range(len(subX)):\n",
        "                    outs, pres = self.forward_eval(subX[i_samp])\n",
        "                    dW, dB = self.backward_eval(outs, pres, subY[i_samp], subX[i_samp])\n",
        "                    for l_idx in range(self.total_layers):\n",
        "                        dw_sum[l_idx] += dW[l_idx]\n",
        "                        db_sum[l_idx] += dB[l_idx]\n",
        "\n",
        "                for l_idx in range(self.total_layers):\n",
        "                    v_w[l_idx] = self.momentum_factor * v_w[l_idx] + self.lr * dw_sum[l_idx]\n",
        "                    self.W_matrices[l_idx] -= v_w[l_idx] + self.weight_decay * self.W_matrices[l_idx]\n",
        "\n",
        "                    v_b[l_idx] = self.momentum_factor * v_b[l_idx] + self.lr * db_sum[l_idx]\n",
        "                    self.B_vectors[l_idx] -= v_b[l_idx]\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def rmsprop_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        RMSProp optimization.\n",
        "        \"\"\"\n",
        "        accum_w = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        accum_b = [np.zeros_like(b) for b in self.B_vectors]\n",
        "        data_count = len(xT)\n",
        "\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for start_idx in range(0, data_count, self.batch_size):\n",
        "                end_idx = start_idx + self.batch_size\n",
        "                chunkX = xT[start_idx:end_idx]\n",
        "                chunkY = yT[start_idx:end_idx]\n",
        "\n",
        "                grad_wtemp = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                grad_btemp = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for i_ex in range(len(chunkX)):\n",
        "                    outs, pres = self.forward_eval(chunkX[i_ex])\n",
        "                    dw, db = self.backward_eval(outs, pres, chunkY[i_ex], chunkX[i_ex])\n",
        "                    for ly in range(self.total_layers):\n",
        "                        grad_wtemp[ly] += dw[ly]\n",
        "                        grad_btemp[ly] += db[ly]\n",
        "\n",
        "                for ly in range(self.total_layers):\n",
        "                    accum_w[ly] = self.rms_factor * accum_w[ly] + (1 - self.rms_factor) * (grad_wtemp[ly] ** 2)\n",
        "                    accum_b[ly] = self.rms_factor * accum_b[ly] + (1 - self.rms_factor) * (grad_btemp[ly] ** 2)\n",
        "\n",
        "                    self.W_matrices[ly] -= (self.lr * grad_wtemp[ly]) / (np.sqrt(accum_w[ly] + 1e-4)) \\\n",
        "                                           + self.weight_decay * self.W_matrices[ly]\n",
        "                    self.B_vectors[ly]  -= (self.lr * grad_btemp[ly]) / (np.sqrt(accum_b[ly] + 1e-4))\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def adam_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        Adam optimization.\n",
        "        \"\"\"\n",
        "        mw = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        vw = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        mb = [np.zeros_like(b) for b in self.B_vectors]\n",
        "        vb = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "        dcount = len(xT)\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for start in range(0, dcount, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                x_part = xT[start:end]\n",
        "                y_part = yT[start:end]\n",
        "\n",
        "                stepGW = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                stepGB = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for i_samp in range(len(x_part)):\n",
        "                    outs, pres = self.forward_eval(x_part[i_samp])\n",
        "                    dW, dB = self.backward_eval(outs, pres, y_part[i_samp], x_part[i_samp])\n",
        "                    for ly in range(self.total_layers):\n",
        "                        stepGW[ly] += dW[ly]\n",
        "                        stepGB[ly] += dB[ly]\n",
        "\n",
        "                for ly in range(self.total_layers):\n",
        "                    mw[ly] = self.adam_beta1 * mw[ly] + (1 - self.adam_beta1) * stepGW[ly]\n",
        "                    vw[ly] = self.adam_beta2 * vw[ly] + (1 - self.adam_beta2) * (stepGW[ly] ** 2)\n",
        "\n",
        "                    mb[ly] = self.adam_beta1 * mb[ly] + (1 - self.adam_beta1) * stepGB[ly]\n",
        "                    vb[ly] = self.adam_beta2 * vb[ly] + (1 - self.adam_beta2) * (stepGB[ly] ** 2)\n",
        "\n",
        "                    mw_hat = mw[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    vw_hat = vw[ly] / (1 - self.adam_beta2**(ep_idx + 1))\n",
        "                    mb_hat = mb[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    vb_hat = vb[ly] / (1 - self.adam_beta2**(ep_idx + 1))\n",
        "\n",
        "                    self.W_matrices[ly] -= (self.lr * mw_hat / (np.sqrt(vw_hat) + self.epsilon_adam)) \\\n",
        "                                           + self.weight_decay * self.W_matrices[ly]\n",
        "                    self.B_vectors[ly]  -= (self.lr * mb_hat / (np.sqrt(vb_hat) + self.epsilon_adam))\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def nadam_optimizer(self, xT, yT, xV, yV):\n",
        "        \"\"\"\n",
        "        NAdam optimization (Adam + Nesterov momentum).\n",
        "        \"\"\"\n",
        "        mW = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        vW = [np.zeros_like(w) for w in self.W_matrices]\n",
        "        mB = [np.zeros_like(b) for b in self.B_vectors]\n",
        "        vB = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "        length_d = len(xT)\n",
        "        for ep_idx in range(self.epochs):\n",
        "            for start_i in range(0, length_d, self.batch_size):\n",
        "                end_i = start_i + self.batch_size\n",
        "                subX = xT[start_i:end_i]\n",
        "                subY = yT[start_i:end_i]\n",
        "\n",
        "                dw_sum = [np.zeros_like(w) for w in self.W_matrices]\n",
        "                db_sum = [np.zeros_like(b) for b in self.B_vectors]\n",
        "\n",
        "                for s_idx in range(len(subX)):\n",
        "                    outs, pres = self.forward_eval(subX[s_idx])\n",
        "                    dW, dB = self.backward_eval(outs, pres, subY[s_idx], subX[s_idx])\n",
        "                    for ly in range(self.total_layers):\n",
        "                        dw_sum[ly] += dW[ly]\n",
        "                        db_sum[ly] += dB[ly]\n",
        "\n",
        "                for ly in range(self.total_layers):\n",
        "                    mW[ly] = self.adam_beta1 * mW[ly] + (1 - self.adam_beta1) * dw_sum[ly]\n",
        "                    vW[ly] = self.adam_beta2 * vW[ly] + (1 - self.adam_beta2) * (dw_sum[ly] ** 2)\n",
        "\n",
        "                    mB[ly] = self.adam_beta1 * mB[ly] + (1 - self.adam_beta1) * db_sum[ly]\n",
        "                    vB[ly] = self.adam_beta2 * vB[ly] + (1 - self.adam_beta2) * (db_sum[ly] ** 2)\n",
        "\n",
        "                    mW_hat = mW[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    vW_hat = vW[ly] / (1 - self.adam_beta2**(ep_idx + 1))\n",
        "                    mB_hat = mB[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    vB_hat = vB[ly] / (1 - self.adam_beta2**(ep_idx + 1))\n",
        "\n",
        "                    corr_w = self.adam_beta1 * mW_hat + (1 - self.adam_beta1) * dw_sum[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    self.W_matrices[ly] -= (self.lr / (np.sqrt(vW_hat) + self.epsilon_adam)) * corr_w \\\n",
        "                                           + self.weight_decay * self.W_matrices[ly]\n",
        "\n",
        "                    corr_b = self.adam_beta1 * mB_hat + (1 - self.adam_beta1) * db_sum[ly] / (1 - self.adam_beta1**(ep_idx + 1))\n",
        "                    self.B_vectors[ly] -= (self.lr / (np.sqrt(vB_hat) + self.epsilon_adam)) * corr_b\n",
        "\n",
        "            self._track_and_log(ep_idx, xT, yT, xV, yV)\n",
        "\n",
        "    def _track_and_log(self, e_index, Xtr, Ytr, Xv, Yv):\n",
        "        \"\"\"\n",
        "        Compute/print/log train and validation metrics at certain epochs.\n",
        "        \"\"\"\n",
        "        if ((self.epochs == 10 and e_index % 2 == 1) or self.epochs == 5):\n",
        "            tr_acc, tr_loss = self._measure_metrics(Xtr, Ytr)\n",
        "            vl_acc, vl_loss = self._measure_metrics(Xv, Yv)\n",
        "            print(f\"[Epoch {e_index}] TrainAcc={tr_acc:.4f}, TrainLoss={tr_loss:.4f}, \"\n",
        "                  f\"ValAcc={vl_acc:.4f}, ValLoss={vl_loss:.4f}\")\n",
        "            wandb.log({\n",
        "                \"train_accuracy\": tr_acc,\n",
        "                \"train_loss\": tr_loss,\n",
        "                \"val_accuracy\": vl_acc,\n",
        "                \"val_loss\": vl_loss,\n",
        "                \"epoch\": e_index\n",
        "            })\n",
        "\n",
        "    def _measure_metrics(self, Xdata, Ydata):\n",
        "        \"\"\"\n",
        "        Calculates accuracy & average loss for the specified loss function.\n",
        "        \"\"\"\n",
        "        correct_sum = 0\n",
        "        total_loss = 0.0\n",
        "        small_eps = 1e-10\n",
        "        data_len = len(Xdata)\n",
        "\n",
        "        for i_eval in range(data_len):\n",
        "            layer_o, _ = self.forward_eval(Xdata[i_eval])\n",
        "            net_out = layer_o[-1]\n",
        "            best_class = np.argmax(net_out)\n",
        "            if best_class == Ydata[i_eval]:\n",
        "                correct_sum += 1\n",
        "\n",
        "            if self.loss_mech == \"cross_entropy\":\n",
        "                prob_c = max(net_out[Ydata[i_eval]][0], small_eps)\n",
        "                total_loss += -math.log10(prob_c)\n",
        "            else:\n",
        "                ideal_vec = np.zeros((self.out_features, 1))\n",
        "                ideal_vec[Ydata[i_eval]] = 1\n",
        "                total_loss += np.sum((net_out - ideal_vec) ** 2)\n",
        "\n",
        "        average_acc = correct_sum / data_len\n",
        "        average_loss = total_loss / data_len\n",
        "        return average_acc, average_loss\n",
        "\n",
        "    def plot_conf_matrix(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Build & log a confusion matrix for test data.\n",
        "        \"\"\"\n",
        "        predictions_list = []\n",
        "        for i_t in range(len(x_test)):\n",
        "            outs, _ = self.forward_eval(x_test[i_t])\n",
        "            pred_label = np.argmax(outs[-1])\n",
        "            predictions_list.append(pred_label)\n",
        "\n",
        "        cmatrix = np.zeros((self.out_features, self.out_features))\n",
        "        for idx, real_class in enumerate(y_test):\n",
        "            predicted_class = predictions_list[idx]\n",
        "            cmatrix[real_class][predicted_class] += 1\n",
        "\n",
        "        class_names = [\n",
        "            'Ankle boot', 'T-shirt/top', 'Dress', 'Pullover',\n",
        "            'Sneaker', 'Sandal', 'Trouser', 'Shirt', 'Coat', 'Bag'\n",
        "        ]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cmatrix, annot=True, fmt='.1f', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(\"Resulting Confusion Matrix\")\n",
        "        plt.xlabel(\"Predicted Category\")\n",
        "        plt.ylabel(\"Actual Category\")\n",
        "        plt.savefig(\"matrix_confusion.png\")\n",
        "        wandb.log({\"ConfusionMatrix\": wandb.Image(\"matrix_confusion.png\")})\n",
        "        plt.show()\n",
        "\n",
        "    # Activation functions & derivatives\n",
        "    def _apply_activation(self, kind, arr):\n",
        "        if kind == \"sigmoid\":\n",
        "            return self._sigmoid(arr)\n",
        "        elif kind == \"tanh\":\n",
        "            return self._tanh(arr)\n",
        "        elif kind == \"relu\":\n",
        "            return self._relu(arr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {kind}\")\n",
        "\n",
        "    def _activate_derivative(self, kind, arr):\n",
        "        if kind == \"sigmoid\":\n",
        "            return self._sigmoid_deriv(arr)\n",
        "        elif kind == \"tanh\":\n",
        "            return self._tanh_deriv(arr)\n",
        "        elif kind == \"relu\":\n",
        "            return self._relu_deriv(arr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation derivative: {kind}\")\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        clipped_z = np.clip(z, -500, 500)\n",
        "        return 1.0 / (1.0 + np.exp(-clipped_z))\n",
        "\n",
        "    def _sigmoid_deriv(self, arr):\n",
        "        s_val = self._sigmoid(arr)\n",
        "        return s_val * (1 - s_val)\n",
        "\n",
        "    def _tanh(self, z):\n",
        "        clipped_z = np.clip(z, -100, 100)\n",
        "        return np.tanh(clipped_z)\n",
        "\n",
        "    def _tanh_deriv(self, arr):\n",
        "        return 1.0 - (self._tanh(arr) ** 2)\n",
        "\n",
        "    def _relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def _relu_deriv(self, arr):\n",
        "        return (arr > 0).astype(arr.dtype)\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        shift_z = z - np.max(z)\n",
        "        exps = np.exp(shift_z)\n",
        "        return exps / np.sum(exps, axis=0)"
      ],
      "metadata": {
        "id": "svBQIhUmLJ7d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFAULT HYPERPARAMETERS & SWEEP CONFIG\n",
        "custom_defaults = {\n",
        "    \"epochs\": 10,\n",
        "    \"hidden_layers\": 3,\n",
        "    \"hl_size\": 128,\n",
        "    \"weight_decay\": 0,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"optimizer\": \"nadam\",\n",
        "    \"batch_size\": 32,\n",
        "    \"initialization\": \"xavier\",\n",
        "    \"activation\": \"relu\",\n",
        "    \"loss\": \"cross_entropy\",\n",
        "    \"wandb_project\": \"DL assignment 1\",\n",
        "    \"wandb_entity\": \"\",\n",
        "    \"momentum_beta\": 0.9,\n",
        "    \"rms_beta\": 0.5,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.9,\n",
        "    \"eps\": 1e-8\n",
        "}"
      ],
      "metadata": {
        "id": "sJvS7sIZMHjw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_sweep = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'sweep-2',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'train_accuracy',\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'hidden_layers': {'values': [3, 4, 5]},\n",
        "        'hl_size': {'values': [32, 64, 128]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learning_rate': {'values': [0.0001, 0.001]},\n",
        "        'optimizer': {'values': ['sgd', 'momentum', 'nestrov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'initialization': {'values': ['random', 'xavier']},\n",
        "        'activation': {'values': ['sigmoid', 'tanh', 'relu']},\n",
        "        'loss': {'values': ['cross_entropy']},\n",
        "        'momentum_beta': {'values': [0.9]},\n",
        "        'rms_beta': {'values': [0.5]},\n",
        "        'beta1': {'values': [0.9]},\n",
        "        'beta2': {'values': [0.9]},\n",
        "        'eps': {'values': [1e-8]},\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "b9LF1BYhMTVJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN ENTRY POINT\n",
        "if __name__ == \"__main__\":\n",
        "    wandb.finish()\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels), sample_set = acquire_fashion_mnist_data()\n",
        "\n",
        "    label_list = [\n",
        "        'Ankle boot', 'T-shirt/top', 'Dress', 'Pullover',\n",
        "        'Sneaker', 'Sandal', 'Trouser', 'Shirt', 'Coat', 'Bag'\n",
        "    ]\n",
        "    show_class_samples(sample_set, label_list)\n",
        "\n",
        "    X_train, Y_train, X_val, Y_val, X_test, Y_test = expand_flatten_normalize(\n",
        "        train_images, train_labels, test_images, test_labels\n",
        "    )\n",
        "\n",
        "    wandb.finish()\n",
        "    wandb.init(project=\"DL assignment 1\")\n",
        "\n",
        "    # Set up the sweep\n",
        "    wandb.finish()\n",
        "    sweep_key = wandb.sweep(sweep=hyper_sweep, project=\"DL assignment 1\")\n",
        "    print(\"Sweep ID:\", sweep_key)\n",
        "\n",
        "    def run_sweep_experiment():\n",
        "        wandb.finish()\n",
        "        run_in = wandb.init(project=\"DL assignment 1\")\n",
        "        config_used = wandb.config\n",
        "        print(\"Running with configuration:\", config_used)\n",
        "\n",
        "        netz = FullyConnectedNet(784, 10, config_used)\n",
        "        netz.training(X_train, Y_train, X_val, Y_val)\n",
        "        # Possibly do netz.plot_conf_matrix(X_test, Y_test) here\n",
        "\n",
        "    wandb.agent(sweep_key, function=run_sweep_experiment, count=50)"
      ],
      "metadata": {
        "id": "lZ-g-9vbMKfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOpLaGXxNNr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}